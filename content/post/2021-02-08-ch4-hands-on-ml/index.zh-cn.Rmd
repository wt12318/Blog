---
title: 【Ch4】Hands on Machine Learning
author: wutao
date: '2021-02-25'
slug: ch4_hands_on_ml
categories:
  - reading notes
tags:
  - ML
  - notes
  - python
image : "image1.jpg"
---

```{r setup, include=FALSE,eval=TRUE}
knitr::opts_chunk$set(collapse = TRUE,comment=">>",
                      engine.path = list(python = 'C:\\Users\\16504\\AppData\\Local\\Programs\\Python\\Python38\\python.exe'))
```

```{r setup2, include=FALSE,eval=FALSE}
knitr::opts_chunk$set(collapse = TRUE,comment=">>",
                      engine.path = list(python = 'C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python39\\python.exe'))
```

本章主要包括：

-   线性回归模型
-   多项式回归模型
-   逻辑回归模型
-   Softmax回归模型
-   一些正则化的技术
-   梯度下降

## 线性回归

一般线性回归的表示行形式为：输入特征的加权求和再加上截距项(或者叫做bias term)

$$
\hat y = \theta_0 + \theta_1x_1+\theta_2x_2+...+\theta_nx_n
$$ ($\hat y$是预测值，n是特征数量，$x_i$是特征值，$\theta_j$是模型参数 )

也可以写成向量形式：

$$
\hat y = h_{\theta}(x)=\theta^TX 
$$ ($\theta$是参数向量，X是输入特征向量)

在第二章中已经讲过衡量一个线性回归模型常用的指标是RMSE，因此我们可以通过最小化RMSE来找到参数$\theta$,为了简化计算，在实际操作中我们是最小化MSE的(MSE最小化，平方根自然也就是最小的)：

$$
MSE(X,h_{\theta})=\frac{1}{m}\sum_{i=1}^{m}(\theta^TX^{(i)}-y^{(i)})^2
$$

求使损失函数最小的$\theta$最直接的方法就是进行数学求解(解析解，也叫normal equation)，MSE的Normal Equation为：

$$
\hat \theta = (X^TX)^{-1}X^Ty
$$ 我们可以来验证一下：

```{python}
##生成数据
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt

x = 2 * np.random.rand(100,1)
y = 4 + 3 * x + np.random.randn(100,1)
plt.plot(x, y, "b.")
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.axis([0, 2, 0, 15])
plt.show()
```

计算$\theta$的Normal equation:

```{python}
x_b = np.c_[np.ones((100,1)),x]##x_0 = 1
theta_best = np.linalg.inv(x_b.T.dot(x_b)).dot(x_b.T).dot(y)

theta_best
```

`np.c_`进行的是增加列的操作(R里面的cbind);`np.ones((100,1))`产生100行1列的矩阵，元素都是1;`np.linalg`是numpy中线性代数模块;`inv`是矩阵求逆方法;`T`是矩阵转置方法;`dot`是矩阵乘法

现在我们使用计算出的$\hat \theta$来预测：

```{python}
x_new = np.array([[0],[2]])
x_new_b = np.c_[np.ones((2,1)),x_new]
y_pre = x_new_b.dot(theta_best)
y_pre
```

```{python}
plt.plot(x_new,y_pre,"r-")
plt.plot(x,y,"b.")
plt.axis([0,2,0,15])
plt.show()
```

在Scikit-Learn中可以使用`LinearRegression`来方便的进行线性回归的计算：

```{python}
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(x,y)
lin_reg.intercept_, lin_reg.coef_

lin_reg.predict(x_new)
```

`LinearRegression`类是基于`scipy.linalg.lstsq`函数的，该函数是通过SVD进行计算pseudoinverse($X^+$)然后再计算$\hat \theta = X^+y$,这样计算有两个好处：pseudoinverse的计算比直接计算矩阵的逆效率更高(why?)；当$X^TX$不可逆的时候Normal Equation是无法计算的，而pseudoinverse是可以计算的

计算Normal Equation的计算复杂度是比较大的(求矩阵的逆的计算复杂度为$O(n^{2.4})$\~$O(n^{3})$,使用SVD方法的计算复杂度为$O(n^2)$)

## 梯度下降

### 数学理论

这一部分参考李宏毅老师的机器学习课程

现在的问题是：找到$\theta^*$：\
$$
\theta^* = argmin_{\theta}L(\theta)
$$ $L(\theta)$是损失函数

现在假设$\theta$由两个参数构成：{$\theta_1$,$\theta_2$},$L(\theta)$的等高线如下图：                    
![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227170016311.png)

给定一个点，我们是否可以在其邻域内找到一个使$L(\theta)$最小的点然后向这个点移动最终到达全局最小点(如上图)；那么怎样找到这个点呢？

这里需要引入**[泰勒级数](https://www.bilibili.com/video/BV1Gx411Y7cz?from=search&seid=4438787146009065334)**的概念：**泰勒级数利用函数在某个点的导数来近似在这个点附近的函数值**,数学表示为：
在$x=x_0$附近有：
$$
h(x) = h(x_0)+h^{'}(x_0)(x-x_0)+\frac{h^{''}(x_0)}{2!}(x-x_0)^2+...
$$
当x接近$x_0$的时候可以将高次式忽略：
$$
h(x) \approx  h(x_0)+h^{'}(x_0)(x-x_0)
$$
对于多个变量也是类似的：

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227180156655.png)

回到上面的问题:

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227180518842.png)

**如果红色的圆圈足够小**，我们就可以使用泰勒级数来近似损失函数：
$$
L(\theta) \approx  L(a,b)+\frac{\partial L(a,b)}{\partial \theta_1}(\theta_1-a)+\frac{\partial L(a,b)}{\partial \theta_2}(\theta_2-b)
$$
令$s=L(a,b)$,$u=\frac{\partial L(a,b)}{\partial \theta_1}$,$v=\frac{\partial L(a,b)}{\partial \theta_2}$,将上式简化：
$$
L(\theta) \approx s + u(\theta_1-a)+v(\theta_2-b)
$$
我们现在的问题就是：在红色的圆圈内找到$\theta_1$和$\theta_2$使得$L(\theta)$最小 

如果使$\theta_1-a=\Delta \theta_1$,$\theta_2-b=\Delta \theta_2$，那么$L(\theta)$就可以表示为两个向量的乘积：

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227182845081.png)

要使$L(\theta)$最小，那么就要使这两个向量反向(并且$(\Delta \theta_1,\Delta \theta_2)$在圆上)：

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227183501439.png)

这个就是梯度下降的形式！
$$
\theta^i = \theta^{i-1} - \eta \bigtriangledown L(\theta^{i-1})
$$

### 梯度下降的注意事项

#### 学习率的调整

学习率($\eta$)是一个重要的超参数，决定了梯度下降的步伐有多大;如果学习率比较小,那么收敛到最小值需要迭代的次数就比较多，如果学习率比较大,那么就可能跳过了最小值，甚至有可能比起始值还要大：

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227185426216.png)

除了手动设定学习率之外，我们还可以使学习率随着训练的进行逐渐减少(在每次迭代时，决定学习率的函数叫做*learning schedule*)

#### 随机梯度下降

上面提到的损失函数都是对所有的训练数据来计算的(所有预测值和真实值的误差和)，而随机梯度下降所使用的计算梯度的函数是随机选取的观测值的预测值和真实值的误差(只看一个点)，更有效率

#### 特征的归一化

下面的图比较形象的表示了归一化对学习的影响：

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227210417449.png)

如果两个特征的范围不一样，那么在更新参数时对损失函数的下降的贡献就会不一样

在Scikit learn中可以使用`SGDRegressor`来进行随机梯度下降求解线性回归模型：

```{python}
from sklearn.linear_model import SGDRegressor
sgd_reg = SGDRegressor(max_iter=1000,tol=1e-3,penalty=None,eta0=0.1)
sgd_reg.fit(x,y.ravel())##ravel将列向量转为一维向量
```
`max_iter`表示epoch的数目(epoch指全部训练数据都被模型"看了"一遍)；`tol`表示如果在某一个epoch上损失函数下降小于tol的数值，则训练停止；`penalty`表示正则化(后面讲);`eta0`表示初始的学习率大小，默认的学习率是:$eta0/pow(t,power\_t)$,power_t的默认值是0.25

## 多项式回归

可以使用线性模型来拟合非线性的数据，一个简单的做法就是将每个特征加上幂次作为新的特征，然后对这些拓展的特征进行训练线性模型，这个技术叫做**多项式回归(polynomial regression)**

```{python}
##模拟数据
m = 100
x = 6 * np.random.rand(m,1) - 3 ##均匀分布
y = 0.5 * x**2 + x + 2 + np.random.randn(m,1)##正态分布

plt.plot(x, y, "b.")
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.axis([-3, 3, 0, 10])
plt.show()
```

使用`PolynomialFeatures`类将特征加上平方后作为新的特征：

```{python}
from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree=2,include_bias=False)
x_poly = poly_features.fit_transform(x)

x[0]
x_poly[0]
```

然后重新训练模型：

```{python}
lin_reg = LinearRegression()
lin_reg.fit(x_poly,y)
lin_reg.intercept_, lin_reg.coef_
```

预测：

```{python}
x_new=np.linspace(-3, 3, 100).reshape(100, 1)
x_new_poly = poly_features.transform(x_new)
y_new = lin_reg.predict(x_new_poly)
plt.plot(x, y, "b.")
plt.plot(x_new, y_new, "r-", linewidth=2, label="Predictions")
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.legend(loc="upper left", fontsize=14)
plt.axis([-3, 3, 0, 10])
plt.show()
```

需要注意的是：`PolynomialFeatures(degree=d)`会将原来的n个特征变成$\frac{(n+d)!}{d!n!}$个特征；比如有两个特征a,b,经过自由度为3的PolynomialFeatures转化后就有10个特征(包括1)

## 学习曲线
