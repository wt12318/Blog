---
title: 【Ch4】Hands on Machine Learning
author: wutao
date: '2021-02-25'
slug: ch4_hands_on_ml
categories:
  - reading notes
tags:
  - ML
  - notes
  - python
image : "image1.jpg"
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>本章主要包括：</p>
<ul>
<li>线性回归模型</li>
<li>多项式回归模型</li>
<li>逻辑回归模型</li>
<li>Softmax回归模型</li>
<li>一些正则化的技术</li>
<li>梯度下降</li>
</ul>
<div id="线性回归" class="section level2">
<h2>线性回归</h2>
<p>一般线性回归的表示行形式为：输入特征的加权求和再加上截距项(或者叫做bias term)</p>
<p><span class="math display">\[
\hat y = \theta_0 + \theta_1x_1+\theta_2x_2+...+\theta_nx_n
\]</span> (<span class="math inline">\(\hat y\)</span>是预测值，n是特征数量，<span class="math inline">\(x_i\)</span>是特征值，<span class="math inline">\(\theta_j\)</span>是模型参数 )</p>
<p>也可以写成向量形式：</p>
<p><span class="math display">\[
\hat y = h_{\theta}(x)=\theta^TX 
\]</span> (<span class="math inline">\(\theta\)</span>是参数向量，X是输入特征向量)</p>
<p>在第二章中已经讲过衡量一个线性回归模型常用的指标是RMSE，因此我们可以通过最小化RMSE来找到参数<span class="math inline">\(\theta\)</span>,为了简化计算，在实际操作中我们是最小化MSE的(MSE最小化，平方根自然也就是最小的)：</p>
<p><span class="math display">\[
MSE(X,h_{\theta})=\frac{1}{m}\sum_{i=1}^{m}(\theta^TX^{(i)}-y^{(i)})^2
\]</span></p>
<p>求使损失函数最小的<span class="math inline">\(\theta\)</span>最直接的方法就是进行数学求解(解析解，也叫normal equation)，MSE的Normal Equation为：</p>
<p><span class="math display">\[
\hat \theta = (X^TX)^{-1}X^Ty
\]</span> 我们可以来验证一下：</p>
<pre class="python"><code>##生成数据
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt

x = 2 * np.random.rand(100,1)
y = 4 + 3 * x + np.random.randn(100,1)
plt.plot(x, y, &quot;b.&quot;)
&gt;&gt; [&lt;matplotlib.lines.Line2D object at 0x00000000125F78E0&gt;]
plt.xlabel(&quot;$x_1$&quot;, fontsize=18)
&gt;&gt; Text(0.5, 0, &#39;$x_1$&#39;)
plt.ylabel(&quot;$y$&quot;, rotation=0, fontsize=18)
&gt;&gt; Text(0, 0.5, &#39;$y$&#39;)
plt.axis([0, 2, 0, 15])
&gt;&gt; (0.0, 2.0, 0.0, 15.0)
plt.show()</code></pre>
<p><img src="/post/2021-02-08-ch4-hands-on-ml/index.zh-cn_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>计算<span class="math inline">\(\theta\)</span>的Normal equation:</p>
<pre class="python"><code>x_b = np.c_[np.ones((100,1)),x]##x_0 = 1
theta_best = np.linalg.inv(x_b.T.dot(x_b)).dot(x_b.T).dot(y)

theta_best
&gt;&gt; array([[4.13110864],
&gt;&gt;        [2.86759098]])</code></pre>
<p><code>np.c_</code>进行的是增加列的操作(R里面的cbind);<code>np.ones((100,1))</code>产生100行1列的矩阵，元素都是1;<code>np.linalg</code>是numpy中线性代数模块;<code>inv</code>是矩阵求逆方法;<code>T</code>是矩阵转置方法;<code>dot</code>是矩阵乘法</p>
<p>现在我们使用计算出的<span class="math inline">\(\hat \theta\)</span>来预测：</p>
<pre class="python"><code>x_new = np.array([[0],[2]])
x_new_b = np.c_[np.ones((2,1)),x_new]
y_pre = x_new_b.dot(theta_best)
y_pre
&gt;&gt; array([[4.13110864],
&gt;&gt;        [9.86629059]])</code></pre>
<pre class="python"><code>plt.plot(x_new,y_pre,&quot;r-&quot;)
&gt;&gt; [&lt;matplotlib.lines.Line2D object at 0x000000001312F8B0&gt;]
plt.plot(x,y,&quot;b.&quot;)
&gt;&gt; [&lt;matplotlib.lines.Line2D object at 0x000000001312FD00&gt;]
plt.axis([0,2,0,15])
&gt;&gt; (0.0, 2.0, 0.0, 15.0)
plt.show()</code></pre>
<p><img src="/post/2021-02-08-ch4-hands-on-ml/index.zh-cn_files/figure-html/unnamed-chunk-1-3.png" width="672" /></p>
<p>在Scikit-Learn中可以使用<code>LinearRegression</code>来方便的进行线性回归的计算：</p>
<pre class="python"><code>from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(x,y)
&gt;&gt; LinearRegression()
lin_reg.intercept_, lin_reg.coef_
&gt;&gt; (array([4.13110864]), array([[2.86759098]]))
lin_reg.predict(x_new)
&gt;&gt; array([[4.13110864],
&gt;&gt;        [9.86629059]])</code></pre>
<p><code>LinearRegression</code>类是基于<code>scipy.linalg.lstsq</code>函数的，该函数是通过SVD进行计算pseudoinverse(<span class="math inline">\(X^+\)</span>)然后再计算<span class="math inline">\(\hat \theta = X^+y\)</span>,这样计算有两个好处：pseudoinverse的计算比直接计算矩阵的逆效率更高(why?)；当<span class="math inline">\(X^TX\)</span>不可逆的时候Normal Equation是无法计算的，而pseudoinverse是可以计算的</p>
<p>计算Normal Equation的计算复杂度是比较大的(求矩阵的逆的计算复杂度为<span class="math inline">\(O(n^{2.4})\)</span>~<span class="math inline">\(O(n^{3})\)</span>,使用SVD方法的计算复杂度为<span class="math inline">\(O(n^2)\)</span>)</p>
</div>
<div id="梯度下降" class="section level2">
<h2>梯度下降</h2>
<div id="数学理论" class="section level3">
<h3>数学理论</h3>
<p>这一部分参考李宏毅老师的机器学习课程</p>
<p>现在的问题是：找到<span class="math inline">\(\theta^*\)</span>：<br />
<span class="math display">\[
\theta^* = argmin_{\theta}L(\theta)
\]</span> <span class="math inline">\(L(\theta)\)</span>是损失函数</p>
<p>现在假设<span class="math inline">\(\theta\)</span>由两个参数构成：{<span class="math inline">\(\theta_1\)</span>,<span class="math inline">\(\theta_2\)</span>},<span class="math inline">\(L(\theta)\)</span>的等高线如下图：<br />
<img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227170016311.png" /></p>
<p>给定一个点，我们是否可以在其邻域内找到一个使<span class="math inline">\(L(\theta)\)</span>最小的点然后向这个点移动最终到达全局最小点(如上图)；那么怎样找到这个点呢？</p>
<p>这里需要引入<strong><a href="https://www.bilibili.com/video/BV1Gx411Y7cz?from=search&amp;seid=4438787146009065334">泰勒级数</a></strong>的概念：<strong>泰勒级数利用函数在某个点的导数来近似在这个点附近的函数值</strong>,数学表示为：
在<span class="math inline">\(x=x_0\)</span>附近有：
<span class="math display">\[
h(x) = h(x_0)+h^{&#39;}(x_0)(x-x_0)+\frac{h^{&#39;&#39;}(x_0)}{2!}(x-x_0)^2+...
\]</span>
当x接近<span class="math inline">\(x_0\)</span>的时候可以将高次式忽略：
<span class="math display">\[
h(x) \approx  h(x_0)+h^{&#39;}(x_0)(x-x_0)
\]</span>
对于多个变量也是类似的：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227180156655.png" /></p>
<p>回到上面的问题:</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227180518842.png" /></p>
<p><strong>如果红色的圆圈足够小</strong>，我们就可以使用泰勒级数来近似损失函数：
<span class="math display">\[
L(\theta) \approx  L(a,b)+\frac{\partial L(a,b)}{\partial \theta_1}(\theta_1-a)+\frac{\partial L(a,b)}{\partial \theta_2}(\theta_2-b)
\]</span>
令<span class="math inline">\(s=L(a,b)\)</span>,<span class="math inline">\(u=\frac{\partial L(a,b)}{\partial \theta_1}\)</span>,<span class="math inline">\(v=\frac{\partial L(a,b)}{\partial \theta_2}\)</span>,将上式简化：
<span class="math display">\[
L(\theta) \approx s + u(\theta_1-a)+v(\theta_2-b)
\]</span>
我们现在的问题就是：在红色的圆圈内找到<span class="math inline">\(\theta_1\)</span>和<span class="math inline">\(\theta_2\)</span>使得<span class="math inline">\(L(\theta)\)</span>最小</p>
<p>如果使<span class="math inline">\(\theta_1-a=\Delta \theta_1\)</span>,<span class="math inline">\(\theta_2-b=\Delta \theta_2\)</span>，那么<span class="math inline">\(L(\theta)\)</span>就可以表示为两个向量的乘积：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227182845081.png" /></p>
<p>要使<span class="math inline">\(L(\theta)\)</span>最小，那么就要使这两个向量反向(并且<span class="math inline">\((\Delta \theta_1,\Delta \theta_2)\)</span>在圆上)：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227183501439.png" /></p>
<p>这个就是梯度下降的形式！
<span class="math display">\[
\theta^i = \theta^{i-1} - \eta \bigtriangledown L(\theta^{i-1})
\]</span></p>
</div>
<div id="梯度下降的注意事项" class="section level3">
<h3>梯度下降的注意事项</h3>
<div id="学习率的调整" class="section level4">
<h4>学习率的调整</h4>
<p>学习率(<span class="math inline">\(\eta\)</span>)是一个重要的超参数，决定了梯度下降的步伐有多大;如果学习率比较小,那么收敛到最小值需要迭代的次数就比较多，如果学习率比较大,那么就可能跳过了最小值，甚至有可能比起始值还要大：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227185426216.png" /></p>
<p>除了手动设定学习率之外，我们还可以使学习率随着训练的进行逐渐减少(在每次迭代时，决定学习率的函数叫做<em>learning schedule</em>)</p>
</div>
<div id="随机梯度下降" class="section level4">
<h4>随机梯度下降</h4>
<p>上面提到的损失函数都是对所有的训练数据来计算的(所有预测值和真实值的误差和)，而随机梯度下降所使用的计算梯度的函数是随机选取的观测值的预测值和真实值的误差(只看一个点)，更有效率</p>
</div>
<div id="特征的归一化" class="section level4">
<h4>特征的归一化</h4>
<p>下面的图比较形象的表示了归一化对学习的影响：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227210417449.png" /></p>
<p>如果两个特征的范围不一样，那么在更新参数时对损失函数的下降的贡献就会不一样</p>
<p>在Scikit learn中可以使用<code>SGDRegressor</code>来进行随机梯度下降求解线性回归模型：</p>
<pre class="python"><code>from sklearn.linear_model import SGDRegressor
sgd_reg = SGDRegressor(max_iter=1000,tol=1e-3,penalty=None,eta0=0.1)
sgd_reg.fit(x,y.ravel())##ravel将列向量转为一维向量
&gt;&gt; SGDRegressor(eta0=0.1, penalty=None)</code></pre>
<p><code>max_iter</code>表示epoch的数目(epoch指全部训练数据都被模型“看了”一遍)；<code>tol</code>表示如果在某一个epoch上损失函数下降小于tol的数值，则训练停止；<code>penalty</code>表示正则化(后面讲);<code>eta0</code>表示初始的学习率大小，默认的学习率是:<span class="math inline">\(eta0/pow(t,power\_t)\)</span>,power_t的默认值是0.25</p>
</div>
</div>
</div>
<div id="多项式回归" class="section level2">
<h2>多项式回归</h2>
<p>可以使用线性模型来拟合非线性的数据，一个简单的做法就是将每个特征加上幂次作为新的特征，然后对这些拓展的特征进行训练线性模型，这个技术叫做<strong>多项式回归(polynomial regression)</strong></p>
<pre class="python"><code>##模拟数据
m = 100
x = 6 * np.random.rand(m,1) - 3 ##均匀分布
y = 0.5 * x**2 + x + 2 + np.random.randn(m,1)##正态分布

plt.plot(x, y, &quot;b.&quot;)
&gt;&gt; [&lt;matplotlib.lines.Line2D object at 0x0000000064F1D130&gt;]
plt.xlabel(&quot;$x_1$&quot;, fontsize=18)
&gt;&gt; Text(0.5, 0, &#39;$x_1$&#39;)
plt.ylabel(&quot;$y$&quot;, rotation=0, fontsize=18)
&gt;&gt; Text(0, 0.5, &#39;$y$&#39;)
plt.axis([-3, 3, 0, 10])
&gt;&gt; (-3.0, 3.0, 0.0, 10.0)
plt.show()</code></pre>
<p><img src="/post/2021-02-08-ch4-hands-on-ml/index.zh-cn_files/figure-html/unnamed-chunk-1-5.png" width="672" /></p>
<p>使用<code>PolynomialFeatures</code>类将特征加上平方后作为新的特征：</p>
<pre class="python"><code>from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree=2,include_bias=False)
x_poly = poly_features.fit_transform(x)

x[0]
&gt;&gt; array([-1.52505781])
x_poly[0]
&gt;&gt; array([-1.52505781,  2.32580131])</code></pre>
<p>然后重新训练模型：</p>
<pre class="python"><code>lin_reg = LinearRegression()
lin_reg.fit(x_poly,y)
&gt;&gt; LinearRegression()
lin_reg.intercept_, lin_reg.coef_
&gt;&gt; (array([1.88676222]), array([[1.00437321, 0.53404555]]))</code></pre>
<p>预测：</p>
<pre class="python"><code>x_new=np.linspace(-3, 3, 100).reshape(100, 1)
x_new_poly = poly_features.transform(x_new)
y_new = lin_reg.predict(x_new_poly)
plt.plot(x, y, &quot;b.&quot;)
&gt;&gt; [&lt;matplotlib.lines.Line2D object at 0x0000000064FDC250&gt;]
plt.plot(x_new, y_new, &quot;r-&quot;, linewidth=2, label=&quot;Predictions&quot;)
&gt;&gt; [&lt;matplotlib.lines.Line2D object at 0x0000000064FDC640&gt;]
plt.xlabel(&quot;$x_1$&quot;, fontsize=18)
&gt;&gt; Text(0.5, 0, &#39;$x_1$&#39;)
plt.ylabel(&quot;$y$&quot;, rotation=0, fontsize=18)
&gt;&gt; Text(0, 0.5, &#39;$y$&#39;)
plt.legend(loc=&quot;upper left&quot;, fontsize=14)
&gt;&gt; &lt;matplotlib.legend.Legend object at 0x0000000064FDCA60&gt;
plt.axis([-3, 3, 0, 10])
&gt;&gt; (-3.0, 3.0, 0.0, 10.0)
plt.show()</code></pre>
<p><img src="/post/2021-02-08-ch4-hands-on-ml/index.zh-cn_files/figure-html/unnamed-chunk-1-7.png" width="672" /></p>
<p>需要注意的是：<code>PolynomialFeatures(degree=d)</code>会将原来的n个特征变成<span class="math inline">\(\frac{(n+d)!}{d!n!}\)</span>个特征；比如有两个特征a,b,经过自由度为3的PolynomialFeatures转化后就有10个特征(包括1)</p>
</div>
<div id="学习曲线" class="section level2">
<h2>学习曲线</h2>
</div>
