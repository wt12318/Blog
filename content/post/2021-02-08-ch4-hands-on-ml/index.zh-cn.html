---
title: 【Ch4】Hands on Machine Learning
author: wutao
date: '2021-02-25'
slug: ch4_hands_on_ml
categories:
  - reading notes
tags:
  - ML
  - notes
  - python
image : "image1.jpg"
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>本章主要包括：</p>
<ul>
<li>线性回归模型</li>
<li>多项式回归模型</li>
<li>逻辑回归模型</li>
<li>Softmax回归模型</li>
<li>一些正则化的技术</li>
<li>梯度下降</li>
</ul>
<div id="线性回归" class="section level2">
<h2>线性回归</h2>
<p>一般线性回归的表示行形式为：输入特征的加权求和再加上截距项(或者叫做bias term)</p>
<p><span class="math display">\[
\hat y = \theta_0 + \theta_1x_1+\theta_2x_2+...+\theta_nx_n
\]</span>
(<span class="math inline">\(\hat y\)</span>是预测值，n是特征数量，<span class="math inline">\(x_i\)</span>是特征值，<span class="math inline">\(\theta_j\)</span>是模型参数 )</p>
<p>也可以写成向量形式：</p>
<p><span class="math display">\[
\hat y = h_{\theta}(x)=\theta^TX 
\]</span>
(<span class="math inline">\(\theta\)</span>是参数向量，X是输入特征向量)</p>
<p>在第二章中已经讲过衡量一个线性回归模型常用的指标是RMSE，因此我们可以通过最小化RMSE来找到参数<span class="math inline">\(\theta\)</span>,为了简化计算，在实际操作中我们是最小化MSE的(MSE最小化，平方根自然也就是最小的)：</p>
<p><span class="math display">\[
MSE(X,h_{\theta})=\frac{1}{m}\sum_{i=1}^{m}(\theta^TX^{(i)}-y^{(i)})^2
\]</span>
求使损失函数最小的<span class="math inline">\(\theta\)</span>最直接的方法就是进行数学求解(解析解，也叫normal equation)，MSE的Normal Equation为：</p>
<p><span class="math display">\[
\hat \theta = (X^TX)^{-1}X^Ty
\]</span>
我们可以来验证一下：</p>
<pre class="python"><code>##生成数据
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt

x = 2 * np.random.rand(100,1)
y = 4 + 3 * x + np.random.randn(100,1)
plt.plot(x, y, &quot;b.&quot;)
plt.xlabel(&quot;$x_1$&quot;, fontsize=18)
plt.ylabel(&quot;$y$&quot;, rotation=0, fontsize=18)
plt.axis([0, 2, 0, 15])
&gt;&gt; (0.0, 2.0, 0.0, 15.0)
plt.show()</code></pre>
<p><img src="/post/2021-02-08-ch4-hands-on-ml/index.zh-cn_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>计算<span class="math inline">\(\theta\)</span>的Normal equation:</p>
<pre class="python"><code>x_b = np.c_[np.ones((100,1)),x]##x_0 = 1
theta_best = np.linalg.inv(x_b.T.dot(x_b)).dot(x_b.T).dot(y)

theta_best
&gt;&gt; array([[4.39106164],
&gt;&gt;        [2.7759147 ]])</code></pre>
<p><code>np.c_</code>进行的是增加列的操作(R里面的cbind);<code>np.ones((100,1))</code>产生100行1列的矩阵，元素都是1;<code>np.linalg</code>是numpy中线性代数模块;<code>inv</code>是矩阵求逆方法;<code>T</code>是矩阵转置方法;<code>dot</code>是矩阵乘法</p>
<p>现在我们使用计算出的<span class="math inline">\(\hat \theta\)</span>来预测：</p>
<pre class="python"><code>x_new = np.array([[0],[2]])
x_new_b = np.c_[np.ones((2,1)),x_new]
y_pre = x_new_b.dot(theta_best)
y_pre
&gt;&gt; array([[4.39106164],
&gt;&gt;        [9.94289105]])</code></pre>
<pre class="python"><code>plt.plot(x_new,y_pre,&quot;r-&quot;)
plt.plot(x,y,&quot;b.&quot;)
plt.axis([0,2,0,15])
&gt;&gt; (0.0, 2.0, 0.0, 15.0)
plt.show()</code></pre>
<p><img src="/post/2021-02-08-ch4-hands-on-ml/index.zh-cn_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>在Scikit-Learn中可以使用<code>LinearRegression</code>来方便的进行线性回归的计算：</p>
<pre class="python"><code>from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(x,y)
&gt;&gt; LinearRegression()
lin_reg.intercept_, lin_reg.coef_
&gt;&gt; (array([4.39106164]), array([[2.7759147]]))
lin_reg.predict(x_new)
&gt;&gt; array([[4.39106164],
&gt;&gt;        [9.94289105]])</code></pre>
<p><code>LinearRegression</code>类是基于<code>scipy.linalg.lstsq</code>函数的，该函数是通过SVD进行计算pseudoinverse(<span class="math inline">\(X^+\)</span>)然后再计算<span class="math inline">\(\hat \theta = X^+y\)</span>,这样计算有两个好处：pseudoinverse的计算比直接计算矩阵的逆效率更高(why?)；当<span class="math inline">\(X^TX\)</span>不可逆的时候Normal Equation是无法计算的，而pseudoinverse是可以计算的</p>
<p>计算Normal Equation的计算复杂度是比较大的(求矩阵的逆的计算复杂度为<span class="math inline">\(O(n^{2.4})\)</span>~<span class="math inline">\(O(n^{3})\)</span>,使用SVD方法的计算复杂度为<span class="math inline">\(O(n^2)\)</span>)</p>
</div>
