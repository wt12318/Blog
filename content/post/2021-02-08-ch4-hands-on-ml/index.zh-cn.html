---
title: 【Ch4】Hands on Machine Learning
author: wutao
date: '2021-02-25'
slug: ch4_hands_on_ml
categories:
  - reading notes
tags:
  - ML
  - notes
  - python
image : "image1.jpg"
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>本章主要包括：</p>
<ul>
<li>线性回归模型</li>
<li>多项式回归模型</li>
<li>逻辑回归模型</li>
<li>Softmax回归模型</li>
<li>一些正则化的技术</li>
<li>梯度下降</li>
</ul>
<div id="线性回归" class="section level2">
<h2>线性回归</h2>
<p>一般线性回归的表示行形式为：输入特征的加权求和再加上截距项(或者叫做bias term)</p>
<p><span class="math display">\[
\hat y = \theta_0 + \theta_1x_1+\theta_2x_2+...+\theta_nx_n
\]</span> (<span class="math inline">\(\hat y\)</span>是预测值，n是特征数量，<span class="math inline">\(x_i\)</span>是特征值，<span class="math inline">\(\theta_j\)</span>是模型参数 )</p>
<p>也可以写成向量形式：</p>
<p><span class="math display">\[
\hat y = h_{\theta}(x)=\theta^TX 
\]</span> (<span class="math inline">\(\theta\)</span>是参数向量，X是输入特征向量)</p>
<p>在第二章中已经讲过衡量一个线性回归模型常用的指标是RMSE，因此我们可以通过最小化RMSE来找到参数<span class="math inline">\(\theta\)</span>,为了简化计算，在实际操作中我们是最小化MSE的(MSE最小化，平方根自然也就是最小的)：</p>
<p><span class="math display">\[
MSE(X,h_{\theta})=\frac{1}{m}\sum_{i=1}^{m}(\theta^TX^{(i)}-y^{(i)})^2
\]</span></p>
<p>求使损失函数最小的<span class="math inline">\(\theta\)</span>最直接的方法就是进行数学求解(解析解，也叫normal equation)，MSE的Normal Equation为：</p>
<p><span class="math display">\[
\hat \theta = (X^TX)^{-1}X^Ty
\]</span> 我们可以来验证一下：</p>
<pre class="python"><code>##生成数据
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt

x = 2 * np.random.rand(100,1)
y = 4 + 3 * x + np.random.randn(100,1)
plt.plot(x, y, &quot;b.&quot;)
&gt;&gt; [&lt;matplotlib.lines.Line2D object at 0x0000000062B378E0&gt;]
plt.xlabel(&quot;$x_1$&quot;, fontsize=18)
&gt;&gt; Text(0.5, 0, &#39;$x_1$&#39;)
plt.ylabel(&quot;$y$&quot;, rotation=0, fontsize=18)
&gt;&gt; Text(0, 0.5, &#39;$y$&#39;)
plt.axis([0, 2, 0, 15])
&gt;&gt; (0.0, 2.0, 0.0, 15.0)
plt.show()</code></pre>
<p><img src="/post/2021-02-08-ch4-hands-on-ml/index.zh-cn_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>计算<span class="math inline">\(\theta\)</span>的Normal equation:</p>
<pre class="python"><code>x_b = np.c_[np.ones((100,1)),x]##x_0 = 1
theta_best = np.linalg.inv(x_b.T.dot(x_b)).dot(x_b.T).dot(y)

theta_best
&gt;&gt; array([[4.2934393 ],
&gt;&gt;        [2.85930621]])</code></pre>
<p><code>np.c_</code>进行的是增加列的操作(R里面的cbind);<code>np.ones((100,1))</code>产生100行1列的矩阵，元素都是1;<code>np.linalg</code>是numpy中线性代数模块;<code>inv</code>是矩阵求逆方法;<code>T</code>是矩阵转置方法;<code>dot</code>是矩阵乘法</p>
<p>现在我们使用计算出的<span class="math inline">\(\hat \theta\)</span>来预测：</p>
<pre class="python"><code>x_new = np.array([[0],[2]])
x_new_b = np.c_[np.ones((2,1)),x_new]
y_pre = x_new_b.dot(theta_best)
y_pre
&gt;&gt; array([[ 4.2934393 ],
&gt;&gt;        [10.01205171]])</code></pre>
<pre class="python"><code>plt.plot(x_new,y_pre,&quot;r-&quot;)
&gt;&gt; [&lt;matplotlib.lines.Line2D object at 0x000000006316F910&gt;]
plt.plot(x,y,&quot;b.&quot;)
&gt;&gt; [&lt;matplotlib.lines.Line2D object at 0x000000006316FD60&gt;]
plt.axis([0,2,0,15])
&gt;&gt; (0.0, 2.0, 0.0, 15.0)
plt.show()</code></pre>
<p><img src="/post/2021-02-08-ch4-hands-on-ml/index.zh-cn_files/figure-html/unnamed-chunk-1-3.png" width="672" /></p>
<p>在Scikit-Learn中可以使用<code>LinearRegression</code>来方便的进行线性回归的计算：</p>
<pre class="python"><code>from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(x,y)
&gt;&gt; LinearRegression()
lin_reg.intercept_, lin_reg.coef_
&gt;&gt; (array([4.2934393]), array([[2.85930621]]))
lin_reg.predict(x_new)
&gt;&gt; array([[ 4.2934393 ],
&gt;&gt;        [10.01205171]])</code></pre>
<p><code>LinearRegression</code>类是基于<code>scipy.linalg.lstsq</code>函数的，该函数是通过SVD进行计算pseudoinverse(<span class="math inline">\(X^+\)</span>)然后再计算<span class="math inline">\(\hat \theta = X^+y\)</span>,这样计算有两个好处：pseudoinverse的计算比直接计算矩阵的逆效率更高(why?)；当<span class="math inline">\(X^TX\)</span>不可逆的时候Normal Equation是无法计算的，而pseudoinverse是可以计算的</p>
<p>计算Normal Equation的计算复杂度是比较大的(求矩阵的逆的计算复杂度为<span class="math inline">\(O(n^{2.4})\)</span>~<span class="math inline">\(O(n^{3})\)</span>,使用SVD方法的计算复杂度为<span class="math inline">\(O(n^2)\)</span>)</p>
</div>
<div id="梯度下降" class="section level2">
<h2>梯度下降</h2>
<div id="数学理论" class="section level3">
<h3>数学理论</h3>
<p>这一部分参考李宏毅老师的机器学习课程</p>
<p>现在的问题是：找到<span class="math inline">\(\theta^*\)</span>：<br />
<span class="math display">\[
\theta^* = argmin_{\theta}L(\theta)
\]</span> <span class="math inline">\(L(\theta)\)</span>是损失函数</p>
<p>现在假设<span class="math inline">\(\theta\)</span>由两个参数构成：{<span class="math inline">\(\theta_1\)</span>,<span class="math inline">\(\theta_2\)</span>},<span class="math inline">\(L(\theta)\)</span>的等高线如下图：<br />
<img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227170016311.png" /></p>
<p>给定一个点，我们是否可以在其邻域内找到一个使<span class="math inline">\(L(\theta)\)</span>最小的点然后向这个点移动最终到达全局最小点(如上图)；那么怎样找到这个点呢？</p>
<p>这里需要引入<strong><a href="https://www.bilibili.com/video/BV1Gx411Y7cz?from=search&amp;seid=4438787146009065334">泰勒级数</a></strong>的概念：<strong>泰勒级数利用函数在某个点的导数来近似在这个点附近的函数值</strong>,数学表示为：
在<span class="math inline">\(x=x_0\)</span>附近有：
<span class="math display">\[
h(x) = h(x_0)+h^{&#39;}(x_0)(x-x_0)+\frac{h^{&#39;&#39;}(x_0)}{2!}(x-x_0)^2+...
\]</span>
当x接近<span class="math inline">\(x_0\)</span>的时候可以将高次式忽略：
<span class="math display">\[
h(x) \approx  h(x_0)+h^{&#39;}(x_0)(x-x_0)
\]</span>
对于多个变量也是类似的：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227180156655.png" /></p>
<p>回到上面的问题:</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227180518842.png" /></p>
<p><strong>如果红色的圆圈足够小</strong>，我们就可以使用泰勒级数来近似损失函数：
<span class="math display">\[
L(\theta) \approx  L(a,b)+\frac{\partial L(a,b)}{\partial \theta_1}(\theta_1-a)+\frac{\partial L(a,b)}{\partial \theta_2}(\theta_2-b)
\]</span>
令<span class="math inline">\(s=L(a,b)\)</span>,<span class="math inline">\(u=\frac{\partial L(a,b)}{\partial \theta_1}\)</span>,<span class="math inline">\(v=\frac{\partial L(a,b)}{\partial \theta_2}\)</span>,将上式简化：
<span class="math display">\[
L(\theta) \approx s + u(\theta_1-a)+v(\theta_2-b)
\]</span>
我们现在的问题就是：在红色的圆圈内找到<span class="math inline">\(\theta_1\)</span>和<span class="math inline">\(\theta_2\)</span>使得<span class="math inline">\(L(\theta)\)</span>最小</p>
<p>如果使<span class="math inline">\(\theta_1-a=\Delta \theta_1\)</span>,<span class="math inline">\(\theta_2-b=\Delta \theta_2\)</span>，那么<span class="math inline">\(L(\theta)\)</span>就可以表示为两个向量的乘积：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227182845081.png" /></p>
<p>要使<span class="math inline">\(L(\theta)\)</span>最小，那么就要使这两个向量反向(并且<span class="math inline">\((\Delta \theta_1,\Delta \theta_2)\)</span>在圆上)：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227183501439.png" /></p>
<p>这个就是梯度下降的形式！
<span class="math display">\[
\theta^i = \theta^{i-1} - \eta \bigtriangledown L(\theta^{i-1})
\]</span></p>
</div>
<div id="梯度下降的注意事项" class="section level3">
<h3>梯度下降的注意事项</h3>
<div id="学习率的调整" class="section level4">
<h4>学习率的调整</h4>
<p>学习率(<span class="math inline">\(\eta\)</span>)是一个重要的超参数，决定了梯度下降的步伐有多大;如果学习率比较小,那么收敛到最小值需要迭代的次数就比较多，如果学习率比较大,那么就可能跳过了最小值，甚至有可能比起始值还要大：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227185426216.png" /></p>
<p>除了手动设定学习率之外，我们还可以使学习率随着训练的进行逐渐减少(在每次迭代时，决定学习率的函数叫做<em>learning schedule</em>)</p>
</div>
<div id="随机梯度下降" class="section level4">
<h4>随机梯度下降</h4>
<p>上面提到的损失函数都是对所有的训练数据来计算的(所有预测值和真实值的误差和)，而随机梯度下降所使用的计算梯度的函数是随机选取的观测值的预测值和真实值的误差(只看一个点)，更有效率</p>
</div>
<div id="特征的归一化" class="section level4">
<h4>特征的归一化</h4>
<p>下面的图比较形象的表示了归一化对学习的影响：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210227210417449.png" /></p>
<p>如果两个特征的范围不一样，那么在更新参数时对损失函数的下降的贡献就会不一样</p>
<p>在Scikit learn中可以使用<code>SGDRegressor</code>来进行随机梯度下降求解线性回归模型：</p>
<pre class="python"><code>from sklearn.linear_model import SGDRegressor
sgd_reg = SGDRegressor(max_iter=1000,tol=1e-3,penalty=None,eta0=0.1)
sgd_reg.fit(x,y.ravel())##ravel将列向量转为一维向量
&gt;&gt; SGDRegressor(eta0=0.1, penalty=None)</code></pre>
<p><code>max_iter</code>表示epoch的数目(epoch指全部训练数据都被模型“看了”一遍)；<code>tol</code>表示如果在某一个epoch上损失函数下降小于tol的数值，则训练停止；<code>penalty</code>表示正则化(后面讲);<code>eta0</code>表示初始的学习率大小，默认的学习率是:<span class="math inline">\(eta0/pow(t,power\_t)\)</span>,power_t的默认值是0.25</p>
</div>
</div>
</div>
<div id="多项式回归" class="section level2">
<h2>多项式回归</h2>
<p>可以使用线性模型来拟合非线性的数据，一个简单的做法就是将每个特征加上幂次作为新的特征，然后对这些拓展的特征进行训练线性模型，这个技术叫做<strong>多项式回归(polynomial regression)</strong></p>
<pre class="python"><code>##模拟数据
m = 100
np.random.seed(123)
x = 6 * np.random.rand(m,1) - 3 ##均匀分布
y = 0.5 * x**2 + x + 2 + np.random.randn(m,1)##正态分布

plt.plot(x, y, &quot;b.&quot;)
&gt;&gt; [&lt;matplotlib.lines.Line2D object at 0x000000001307D310&gt;]
plt.xlabel(&quot;$x_1$&quot;, fontsize=18)
&gt;&gt; Text(0.5, 0, &#39;$x_1$&#39;)
plt.ylabel(&quot;$y$&quot;, rotation=0, fontsize=18)
&gt;&gt; Text(0, 0.5, &#39;$y$&#39;)
plt.axis([-3, 3, 0, 10])
&gt;&gt; (-3.0, 3.0, 0.0, 10.0)
plt.show()</code></pre>
<p><img src="/post/2021-02-08-ch4-hands-on-ml/index.zh-cn_files/figure-html/unnamed-chunk-1-5.png" width="672" /></p>
<p>使用<code>PolynomialFeatures</code>类将特征加上平方后作为新的特征：</p>
<pre class="python"><code>from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree=2,include_bias=False)
x_poly = poly_features.fit_transform(x)

x[0]
&gt;&gt; array([1.17881511])
x_poly[0]
&gt;&gt; array([1.17881511, 1.38960507])</code></pre>
<p>然后重新训练模型：</p>
<pre class="python"><code>lin_reg = LinearRegression()
lin_reg.fit(x_poly,y)
&gt;&gt; LinearRegression()
lin_reg.intercept_, lin_reg.coef_
&gt;&gt; (array([2.03146145]), array([[0.95505451, 0.50182851]]))</code></pre>
<p>预测：</p>
<pre class="python"><code>x_new=np.linspace(-3, 3, 100).reshape(100, 1)
x_new_poly = poly_features.transform(x_new)
y_new = lin_reg.predict(x_new_poly)
plt.plot(x, y, &quot;b.&quot;)
&gt;&gt; [&lt;matplotlib.lines.Line2D object at 0x000000001314B580&gt;]
plt.plot(x_new, y_new, &quot;r-&quot;, linewidth=2, label=&quot;Predictions&quot;)
&gt;&gt; [&lt;matplotlib.lines.Line2D object at 0x000000001314B970&gt;]
plt.xlabel(&quot;$x_1$&quot;, fontsize=18)
&gt;&gt; Text(0.5, 0, &#39;$x_1$&#39;)
plt.ylabel(&quot;$y$&quot;, rotation=0, fontsize=18)
&gt;&gt; Text(0, 0.5, &#39;$y$&#39;)
plt.legend(loc=&quot;upper left&quot;, fontsize=14)
&gt;&gt; &lt;matplotlib.legend.Legend object at 0x000000001314BD90&gt;
plt.axis([-3, 3, 0, 10])
&gt;&gt; (-3.0, 3.0, 0.0, 10.0)
plt.show()</code></pre>
<p><img src="/post/2021-02-08-ch4-hands-on-ml/index.zh-cn_files/figure-html/unnamed-chunk-1-7.png" width="672" /></p>
<p>需要注意的是：<code>PolynomialFeatures(degree=d)</code>会将原来的n个特征变成<span class="math inline">\(\frac{(n+d)!}{d!n!}\)</span>个特征；比如有两个特征a,b,经过自由度为3的PolynomialFeatures转化后就有10个特征(包括1),要注意特征爆炸的问题</p>
</div>
<div id="学习曲线" class="section level2">
<h2>学习曲线</h2>
<p>使用高自由度的多项式回归模型可能会在训练集上过拟合，然而简单的线性模型可能是欠拟合的，那么我们该怎样决定模型的复杂程度或者说判断模型是过拟合还是欠拟合呢？</p>
<p>在第二章中，使用了交叉验证的方法来估计模型的泛化能力；如果一个模型在训练集上表现的比较好但是依据交叉验证的指标，其泛化能力比较差(在验证集上表现不好)，那么这个模型就是过拟合；如果一个模型在训练集和验证集上表现都不好，那么这个模型是欠拟合的</p>
<p>另外一个方法就是检查<strong>学习曲线</strong>(learning curves),<strong>学习曲线展示了模型在训练集和验证集上的表现和训练集大小或者训练的迭代次数之间的关系</strong>;要画这个图，需要在不同大小的训练集的子集上训练模型，得到模型的表现指标</p>
<p>我们先来画一个简单线性回归的学习曲线：</p>
<pre class="python"><code>from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

def plot_learning_curves(model,x,y):
  x_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.2)
  train_errors,val_errors = [],[]
  for m in range(1,len(x_train)):
    model.fit(x_train[:m],y_train[:m])
    y_train_predict = model.predict(x_train[:m])
    y_val_predict = model.predict(x_val)
    train_errors.append(mean_squared_error(y_train[:m],y_train_predict))
    val_errors.append(mean_squared_error(y_val,y_val_predict))
  
  plt.plot(np.sqrt(train_errors),&quot;r-+&quot;,linewidth=2,label=&quot;train&quot;)
  plt.plot(np.sqrt(val_errors), &quot;b-&quot;, linewidth=3, label=&quot;val&quot;)
  plt.legend(loc=&quot;upper right&quot;, fontsize=14)  
  plt.xlabel(&quot;Training set size&quot;, fontsize=14)
  plt.ylabel(&quot;RMSE&quot;, fontsize=14) 
    
    
lin_reg = LinearRegression()
plot_learning_curves(lin_reg,x,y)
plt.axis([0, 80, 0, 3]) 
&gt;&gt; (0.0, 80.0, 0.0, 3.0)
plt.show()        </code></pre>
<p><img src="/post/2021-02-08-ch4-hands-on-ml/index.zh-cn_files/figure-html/unnamed-chunk-1-9.png" width="672" /></p>
<p>当只有一两个训练数据的时候，模型拟合的非常好，同时由于训练集较少，泛化能力较弱所以在验证集中表现不好；当训练集逐渐增大，一方面由于数据的噪音，另一方面因为模型是线性的，而数据不是线性的，所以模型在训练集上的误差上升，但是由于训练集增多，泛化能力会一定程度的上升，所以在验证集上的误差降低，最终两者都到达一个平台</p>
<p>这个学习曲线是一个典型的欠拟合的模型的特征：<strong>两个曲线都到达一个平台；并且两者比较接近，都比较高</strong></p>
<p>接下来看一下有10个自由度的多项式回归模型的学习曲线：</p>
<pre class="python"><code>from sklearn.pipeline import Pipeline

polynomial_regression = Pipeline([
        (&quot;poly_features&quot;, PolynomialFeatures(degree=20, include_bias=False)),
        (&quot;lin_reg&quot;, LinearRegression()),
    ])

plot_learning_curves(polynomial_regression, x, y)
plt.axis([0, 80, 0, 3])
&gt;&gt; (0.0, 80.0, 0.0, 3.0)
plt.show()           </code></pre>
<p><img src="/post/2021-02-08-ch4-hands-on-ml/index.zh-cn_files/figure-html/unnamed-chunk-1-11.png" width="672" /></p>
<p>这个学习曲线也有两个特征：</p>
<ul>
<li>在训练集上的误差比上面的线性回归模型要低</li>
<li>在两个曲线间有一个gap，这意味着模型在训练集上比在验证集上的表现要好得多，而这是<strong>过拟合</strong>的特征(可能需要收集更多的数据)</li>
</ul>
</div>
<div id="biasvariance-trade-off" class="section level2">
<h2>BIAS/VARIANCE TRADE-OFF</h2>
</div>
<div id="正则化线性模型" class="section level2">
<h2>正则化线性模型</h2>
<p>在第一章和第二章已经讲过了减少过拟合风险的方法之一就是正则化模型(也就是约束模型)；对于多项式模型最简单的正则化方法就是减少模型的自由度；对于线性模型，正则化一般是通过约束模型的权重来实现，常用的有3种方法：岭回归(Ridge Regression),Lasso回归,弹性网络(Elastic Net)</p>
<div id="岭回归" class="section level3">
<h3>岭回归</h3>
<p>岭回归就是在线性回归的损失函数后面加上了一个正则化的项:</p>
<p><span class="math display">\[
J(\theta) = MSE(\theta) + \alpha \frac{1}{2}\sum_{i=1}^{n}\theta_i^2
\]</span>
加上这一项之后就会使得模型在训练的过程中尽量保持特征权重(<span class="math inline">\(\theta\)</span>)比较小<br />
注意：在岭回归等正则化的模型中，训练时使用的损失函数与计算模型性能时用的指标不一定相同(在分类模型中更是如此)；另外在训练正则化的模型时，对特征一定要归一化</p>
<p>下图，左边是线性回归使用岭正则化，右图是多项式回归使用岭正则化，展示了不同<span class="math inline">\(\alpha\)</span>值时的情况：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210301230247251.png" /></p>
<p>可以看到增加<span class="math inline">\(\alpha\)</span>会是曲线更加平缓(减少了variance但是增加了bias)</p>
<p>对于岭回归，和线性回归一样，可以使用normal equation的方法或者梯度下降的方法求解：</p>
<pre class="python"><code>##Normal equation
from sklearn.linear_model import Ridge

ridge_reg = Ridge(alpha=1,solver=&quot;cholesky&quot;)
ridge_reg.fit(x,y)
&gt;&gt; Ridge(alpha=1, solver=&#39;cholesky&#39;)
ridge_reg.predict([[1.5]])
&gt;&gt; array([[4.58785445]])</code></pre>
<pre class="python"><code>##梯度下降
sgd_reg = SGDRegressor(penalty=&quot;l2&quot;)
sgd_reg.fit(x,y.ravel())
&gt;&gt; SGDRegressor()
sgd_reg.predict([[1.5]])
&gt;&gt; array([4.56218836])</code></pre>
<p>“l2”指的是L2范数(norm);<span class="math inline">\(L_p\)</span>范数的定义为：
<span class="math display">\[
||x||_p = \sqrt[p]{\sum_i |x_i|^p}
\]</span>
因此L2范数为：
<span class="math display">\[
||x||_2 = \sqrt[2]{\sum_i |x_i|^2}
\]</span></p>
<p>所以岭回归的正则化项就是<span class="math inline">\(\alpha 1/2(||w||_2)^2\)</span>,w是<span class="math inline">\(\theta_1\)</span>到<span class="math inline">\(\theta_n\)</span>的参数向量(特征权重)</p>
</div>
<div id="lasso回归" class="section level3">
<h3>Lasso回归</h3>
<p>Lasso的全称为Least Absolute Shrinkage and Selection Operator ,和岭回归类似也是在损失函数后面加上一个正则化项，只不过Lasso加的是L1范数：
<span class="math display">\[
J(\theta) = MSE(\theta) + \alpha\sum_{i=1}^n|\theta_i|
\]</span></p>
</div>
</div>
