---
title: 感知机(perceptron)
author: wutao
date: '2021-03-14'
slug: perceptron
categories:
  - reading notes
tags:
  - ML
  - notes
image : "per.png"
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>机器学习方法都是由3个要素构成的：</p>
<ul>
<li>模型：包含输入空间,输出空间和假设空间(包含所有可能的决策函数)</li>
<li>策略：按照什么样的准则选择最优的模型(损失函数)</li>
<li>算法：如何找到最优模型(最优化问题)</li>
</ul>
<div id="感知机模型" class="section level2">
<h2>感知机模型</h2>
<p>输入空间：<span class="math inline">\(X \in R^n\)</span> (n维实数)<br />
输出空间：<span class="math inline">\(Y = {+1,-1}\)</span><br />
假设空间：</p>
<p><span class="math display">\[
f(x)=sign(w\cdot x+b)=\left\{ 
\begin{matrix}
+1, w\cdot x+b\ge0\\
-1, w\cdot x+b&lt;0 \\
\end{matrix}
\right.
\]</span>
注意：<span class="math inline">\(w,x,b\)</span>都是向量,<span class="math inline">\(w\cdot b\)</span>也就是向量的内积,比如在二维空间中：</p>
<center>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/演示文稿1_01.png" /></p>
</center>
<p>就是要找一个直线<span class="math inline">\(w_1x_1+w_2x_2+b=0\)</span>将点分成两类(这条直线更一般的名称叫做超平面)；另外感知机模型对数据的假设是:数据是线性可分的;比如下图所示的数据所对应的就不是一个线性可分的输入空间</p>
<center>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/Rd9f79181b6f2972e0795a5815e8dc3a0.png" /></p>
</center>
</div>
<div id="学习策略" class="section level2">
<h2>学习策略</h2>
<p>感知机的损失函数为：<strong>误分类点到超平面S的总距离</strong>,通过最小化这个距离得到最优的超平面(超平面的参数就是w和b)</p>
<p>首先我们需要一些基础知识：</p>
<div id="超平面的法向量" class="section level3">
<h3>超平面的法向量</h3>
<p>对于一个超平面S (<span class="math inline">\(w\cdot x+b\)</span>),其法向量为<span class="math inline">\(w\)</span>:</p>
<p>设超平面S上有两个点：A点<span class="math inline">\((x_A)\)</span>和B点<span class="math inline">\((x_B)\)</span>有：</p>
<p><span class="math display">\[
\left\{ \begin{matrix}         
wx_A+b=0\\
wx_B+b=0 \\
\end{matrix}\right. \\
\Rightarrow w(x_A-x_B)=0
\]</span>
因为<span class="math inline">\(x_A-x_B\)</span>是超平面S上的一个向量,两个向量的乘积为0,所以<span class="math inline">\(w\)</span>垂直于S,即<span class="math inline">\(w\)</span>为超平面S的法向量</p>
</div>
<div id="点到超平面的距离" class="section level3">
<h3>点到超平面的距离</h3>
<p>输入空间中任一点<span class="math inline">\(x_0\)</span>到超平面S (<span class="math inline">\(w\cdot x+b\)</span>)的距离d为：</p>
<span class="math display">\[
d = \frac{1}{||w||}|w\cdot x_0+b|
\]</span>
<center>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210314141051985.png" /></p>
</center>
<p>设点<span class="math inline">\(x_0\)</span>在S上投影为<span class="math inline">\(x_1\)</span>,则<span class="math inline">\(w\cdot x_1+b=0\)</span>;由于向量<span class="math inline">\(\vec {x_1x_0}\)</span>与S的法向量<span class="math inline">\(w\)</span>平行,所以：</p>
<p><span class="math display">\[
|\vec w\cdot \vec{x_1x_0}|=||\vec w||×||\vec{x_1x_0}||cos&lt;\vec w,\vec{x_1x_0}&gt;=||\vec w||×||\vec{x_1x_0}||=||\vec w||d
\]</span>
对于<span class="math inline">\(\vec w\cdot \vec{x_1x_0}\)</span>又有(假设<span class="math inline">\(w\)</span>和<span class="math inline">\(x\)</span>都是N维的向量,上面的图只是一个3维的例子)：</p>
<p><span class="math display">\[
\vec w\cdot \vec{x_1x_0}=w^1 (x_1^1-x_0^1)+w^2(x_1^2-x_0^2)+...+w^N(x_1^N-x_0^N) \\ =w^1x_1^1+w^2x_1^2+...+w^Nx_1^N-(w^1x_0^1+w^2x_0^2+...+w^Nx_0^N) \\ =-b-(w^1x_0^1+w^2x_0^2+...+w^Nx_0^N)
\]</span>
因此由上面两个式子,可以得出：</p>
<p><span class="math display">\[
||w||d=|-b-(w^1x_0^1+w^2x_0^2+...+w^Nx_0^N)|=|w\cdot x_0 +b|\\
\Rightarrow d=\frac{|w\cdot x_0 +b|}{||w||} 
\]</span></p>
<p>回到感知机模型中,因为误分类点<span class="math inline">\(w\cdot x+b\)</span>和类标签的符号是相反的(当<span class="math inline">\(w\cdot x+b\)</span>大于0时,误分类的类标签是-1;当<span class="math inline">\(w\cdot x+b\)</span>小于0时,误分类的类标签是+1),所以误分类点到超平面S的距离也可以表示为:</p>
<p><span class="math display">\[
d_i = \frac{-y_i(w\cdot x_i+b)}{||w||}
\]</span>
误分类点的总距离为：</p>
<p><span class="math display">\[
-\frac{1}{||w||}\sum_{x_i\in M}y_i(w\cdot x_i+b),M为误分类点的集合
\]</span>
所以感知机的损失函数为：</p>
<p><span class="math display">\[
L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)
\]</span></p>
</div>
</div>
<div id="学习算法" class="section level2">
<h2>学习算法</h2>
<p>可以使用梯度下降或者随机梯度下降的方法来求解使损失函数最小化时的参数<span class="math inline">\(w,b\)</span></p>
<p>损失函数<span class="math inline">\(L(w,b)\)</span>的梯度为：</p>
<p><span class="math display">\[
\nabla_{w}L(w,b)=\frac{\partial L(w,b)}{\partial w}=-\sum_{x_i\in M}y_ix_i \\
\nabla_{b}L(w,b)=\frac{\partial L(w,b)}{\partial b}=-\sum_{x_i\in M}y_i
\]</span></p>
<p>所以按照梯度下降法,对每个误分类点更新w,b:</p>
<p><span class="math display">\[
\left\{ \begin{matrix}         
w := w+\eta\sum_iy_ix_i\\
b := b+\eta\sum_iy_i\\
\end{matrix}\right. \\
\]</span>
<span class="math inline">\(\eta\)</span>是学习率;在实际应用中一般选择使用随机梯度下降:</p>
<p><span class="math display">\[
\left\{ \begin{matrix}         
w := w+\eta y_ix_i\\
b := b+\eta y_i\\
\end{matrix}\right. \\
\]</span>
感知机的学习算法(随机梯度下降法)的步骤为:</p>
<ul>
<li>选取初值<span class="math inline">\(w_0,b_0\)</span></li>
<li>在训练集中选取数据<span class="math inline">\((x_i,y_i)\)</span></li>
<li>如果选取的点是误分类点,也就是说<span class="math inline">\(y_i(w\cdot x_i+b)\le0\)</span>,按照上式对参数进行更新</li>
<li>转至第二步,直到训练集中没有误分类点</li>
</ul>
</div>
<div id="算法收敛性" class="section level2">
<h2>算法收敛性</h2>
<p>证明如下的定理：</p>
<p>设训练数据集<span class="math inline">\(T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}\)</span>是线性可分的：</p>
<ol style="list-style-type: decimal">
<li>存在满足条件<span class="math inline">\(||\hat w_{opt}||\)</span>=1的超平面<span class="math inline">\(\hat w_{opt} \cdot \hat x=w_{opt}\cdot x+b_{opt}=0\)</span>将数据集完全正确分开,且存在<span class="math inline">\(r&gt;0\)</span>,对所有的<span class="math inline">\(i=1,2,..,N\)</span>有：
<span class="math display">\[
y_i(\hat w_{opt} \cdot \hat x)=y_i(w_{opt}\cdot x+b_{opt})\ge r
\]</span></li>
<li>令<span class="math inline">\(R=\max||\hat x_i||\)</span>,则感知机在训练集上的误分类次数k满足不等式:
<span class="math display">\[
k \le (\frac{R}{r})^2
\]</span></li>
</ol>
<p>首先为了方便,将b放进了w和x中,也就是:
<span class="math display">\[
\hat w=(w^T,b)^T \ ,\hat x = (x^T,1)^T
\]</span>
先证明1：<br />
由于数据集是线性可分的,肯定存在一个超平面将数据集完全分开,即对<span class="math inline">\(i=1,2,...,N\)</span>,都有：
<span class="math display">\[
y_i(\hat w_{opt} \cdot \hat x)&gt;0
\]</span></p>
<p>因此只需要r为<span class="math inline">\(y_i(\hat w_{opt} \cdot \hat x)\)</span>的最小值,就会有：
<span class="math display">\[
y_i(\hat w_{opt} \cdot \hat x) \ge r
\]</span></p>
<p>再来看2：</p>
<p><img src="https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/2021-03-14_16-08-55_00.png" /></p>
<p>也就是说误分类的次数是有上界的,经过有限次搜索肯定是可以找到将训练集完全分开的超平面</p>
</div>
<div id="sci-kit-learn" class="section level2">
<h2>Sci-kit learn</h2>
<p>scikit learn 中的Perceptron类和SGDClassifier类都可以进行感知机模型的计算：</p>
<pre class="python"><code>from sklearn.datasets import load_digits
from sklearn.linear_model import Perceptron

X, y = load_digits(return_X_y=True)
clf = Perceptron(random_state=0)
##也可以使用SGDClassifier(loss=&quot;perceptron&quot;, eta0=1, learning_rate=&quot;constant&quot;, penalty=None)
clf.fit(X, y)
&gt;&gt; Perceptron()
clf.score(X, y)
&gt;&gt; 0.9393433500278241</code></pre>
</div>
