---
title: 集成学习
author: wutao
date: '2021-03-09'
slug: ensemble_and_rf
categories:
  - reading notes
tags:
  - notes
  - python
image : "em.png"
---

```{r setup2, include=FALSE,eval=TRUE}
knitr::opts_chunk$set(collapse = TRUE,comment=">>",
                      engine.path = list(python = 'C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python39\\python.exe'))
```

集成学习就是将一系列的预测器按照某种方式聚合在一起，从而期望结果比单个预测器的效果要好；比如我们可以在训练集的不同的随机选取子集上训练一系列的决策树分类器，然后获取所有树的预测，对于某一个实例，将其多数分类器预测的结果作为最终的预测结果(这种集成学习方法也叫做随机森林)

## Voting Classifiers

上面那个例子就是一个多数表决分类器( majority-vote classifier)，更一般的说就是在训练集上训练多个不同的模型，最后对某个实例的预测是基于所有模型预测值的"投票结果"来决定，多数表决分类器又叫硬投票分类器(hard voting classifier)：

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210309170505986.png)

为什么集成学习比单个的(弱)学习器的效果要好？可以从下面这个例子来理解：

假设我们有一个硬币,每次抛硬币有51%的可能是正面朝上,有49%的可能是反面朝上;扔1000次,大部分朝上(朝上的硬币数目大于500)的概率为：

$$
P(X>500) = 1-binom(500,1000,0.51)
$$
在R里面计算得到概率为：
```{r}
1-pbinom(500,1000,0.51)
```

同理在扔10000次后，得到硬币大部分朝上的概率为：
```{r}
1-pbinom(5000,10000,0.51)
```

进行的实验次数越多，这个概率就越大(大数定理)

现在我们想像：有1000个分类器,每个分类器正确预测的概率为51%,如果我们按照多数投票规则来整合1000个分类器，那么准确率可以达到72%(有超过一半预测的是正确的概率);但是这个前提是这些分类器是相互独立的,但在实际情况中很难保证这一点(一种增强独立性的策略就是使用多个非常不同的分类器)
