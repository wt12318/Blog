---
title: 决策树
author: wutao
date: '2021-03-04'
slug: ch6_hands_on_ML
categories:
  - python
  - reading notes
tags:
  - notes
image : "image1.jpg"
---

```{r setup, include=FALSE,eval=TRUE}
knitr::opts_chunk$set(collapse = TRUE,comment=">>",
                      engine.path = list(python = 'C:\\Users\\16504\\AppData\\Local\\Programs\\Python\\Python38\\python.exe'))
```

决策树是一种多能的机器学习算法，可以处理分类，回归，甚至多输出问题(见第二章)

参考资料：https://www.bilibili.com/video/BV1ut41197F6?from=search&seid=9344266940719140153

https://www.bilibili.com/video/BV1ZK4y1b7Xt

https://www.bilibili.com/video/BV1MA411J7wm

李航统计学习

## 训练和可视化决策树

首先在iris数据集上训练一个决策树模型并可视化：

```{python}
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from matplotlib import pyplot as plt

iris = load_iris()
x = iris.data[:,2:]##取petal length和width变量
y = iris.target

tree_clf = DecisionTreeClassifier(max_depth=2)
tree_clf.fit(x,y)
```

Graphviz是一个开源的图（Graph）可视化软件，采用抽象的图和网络来表示结构化的信息。在数据科学领域，Graphviz的一个用途就是实现决策树可视化,因此我们需要使用`export_graphviz()`将树结构导出为一个`.dot`文件

```{python}
from sklearn.tree import export_graphviz
from graphviz import Source

export_graphviz(
  tree_clf,
  out_file="../test/iris_tree.dot",
  feature_names=iris.feature_names[2:],
  class_names=iris.target_names,
  rounded=True,
  filled=True,
  special_characters=True
)
```

然后需要下载[Graphviz](https://www.graphviz.org/download/),打开powershell：

```{R eval=FALSE}
dot -Tpng iris_tree.dot -o iris_tree.png
```

<center>

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/iris_tree.png)

</center>


## 理论

决策树可以用来处理分类和回归任务，主要思想就是：根据特征对数据集进行划分，决策树的学习分成3个步骤：

- 特征选择
- 生成决策树
- 决策树的修剪(正则化)

### 特征选择

特征选择的就是选择对训练数据有较好分类能力的特征，也就是说通过某个特征将数据集分成若干子集，这些子集中数据的一致性(纯度)应该比原来的数据集要高；在决策树中使用熵来表示这个纯度

对离散型随机变量X，其概率分布为：

$$
P(X=x_i)=p_i,i=1,2,...,n
$$
则X的熵定义为：

$$
H(X) = - \sum_{i=1}^np_ilog_2p_i 
$$
设有随机变量X,Y,其联合概率分布为：

$$
P(X=x_i,Y=y_j)=p_{ij},i=1,2,...,n ;j=1,2,...,m
$$

条件熵为在X给定的条件下Y的条件概率分布的熵对X的数学期望:

$$
H(Y|X) = \sum_{i=1}^np_iH(Y|X=x_i)\\
p_i=P(X=x_i),i=1,2,...,n
$$
由实际数据计算得到的熵和条件熵叫做经验熵和经验条件熵；设数据集为D,根据特征A将数据集分成若干个子集$D_i$,那么D的经验熵($H(D)$)和给定A的条件下D的经验条件熵($H(D|A)$)为:

$$
H(D)=-\sum_{k=1}^K\frac{|D_k|}{|D|}log_2\frac{|D_k|}{|D|},\\
H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|}\\
$$
$|D_k|$表示k类样本的数目,$|D|$是总的样本数,$|D_{ik}|$表示在第i个子集中k类样本的数目,$|D_i|$表示第i个子集的样本数

一个好的分类特征应该是：根据这个特征划分的数据集后的熵应该比原来数据集的熵要低,因此定义信息增益$g(D,A)$为：

$$
g(D,A)=H(D)-H(D|A)
$$

所以根据信息增益来选择特征：**对训练集(或子集)计算每个特征的信息增益，选择信息增益最大的特征来划分数据集**

信息增益计算的是绝对值，因此对取值较多的特征有倾向性(取值越多,加和也越大),所以将信息增益除以该特征的经验熵来标准化信息增益，得到信息增益比:

$$
g_k(D,A)=\frac{g(D,A)}{H_A(D)},H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}
$$
n表示特征A可以取值的个数(A的水平)

### 生成决策树

生成决策树的算法有3种：ID3,C4.5和CRAT，CART算法比较特殊，后面单独讲；前两种算法都只可以用来分类，CART既可以分类也可以回归

ID3算法在决策树的各个节点上应用信息增益法则选择特征，递归构建决策树：从根节点开始，对节点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，由该特征的不同取值建立子节点，再对子节点递归地调用以上方法构建决策树，直到所有特征的信息增益都很小或者没有特征可以选择为止  

C4.5算法和ID3的区别在于使用信息增益比来选择特征

### 决策树的剪枝

在生成决策树的过程中是以尽可能的准确分类为标准，但是这样往往会出现过拟合的情况，为了避免过拟合，需要限制模型的自由度，即对模型进行正则化约束，在决策树模型里面就是剪枝

决策树的剪枝是通过最小化损失函数来实现；决策树学习的损失函数为：

$$
C_{\alpha}(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|\\
H_t(T)=-\sum_k^K\frac{N_{tk}}{N_t}log2\frac{N_{tk}}{N_t}
$$
其中t表示叶节点,|T|是叶节点个数,$N_t$是t叶节点的样本数,$N_{tk}$是t叶节点中k类样本的个数

将损失函数的第一项记作$C(T)$,

$$
C(T)=\sum_{t=1}^{|T|}N_tH_t(T)=-\sum_{t=1}^{|T|}N_t\sum_k^K\frac{N_{tk}}{N_t}log2\frac{N_{tk}}{N_t}=-\sum_{t=1}^{|T|}\sum_k^KN_{tk}log2\frac{N_{tk}}{N_t}
$$

损失函数可以写成：

$$
C_{\alpha}(T)=C(T)+\alpha|T|
$$

$C(T)$表示模型对数据的拟合程度(如果完全拟合，那么经验熵就为为0)，|T|表示模型的复杂度(叶子节点的多少)，$\alpha$的作用就是在两者间平衡(对模型复杂度有个惩罚)

决策树剪枝的过程为：从下往上进行回缩，如果回缩前的模型为$T_A$,回缩后的模型为$T_B$:

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210307192547332.png)

如果有：

$$
C_{\alpha}(T_B) \leq C_{\alpha}(T_A)
$$

那么就进行回缩剪枝，将父节点变为叶节点


### CART算法

CART的全称为classification and regression tree,可以用来处理**分类和回归**任务，得到的决策树是二叉树，内部节点的取值只有是和否,左分支为"是"的分支,右分支为"否"的分支

#### 分类

CART算法使用**基尼指数**作为最优特征的选择依据，而不是信息增益

在分类问题中，假设有K个类，样本点属于第k类的概率为$p_k$,那么概率分布的基尼指数为：

$$
Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2
$$
对于给定的样本集合D，基尼指数为：
$$
Gini(D)=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2
$$

如果数据集D可以根据特征A的某个值分割成D1和D2两个部分，则在特征A的条件下，集合D的基尼指数为：

$$
Gini(D,A)=\frac{|C_1|}{|D|}Gini(D_1)+\frac{|C_2|}{|D|}Gini(D_2)
$$
因此CART算法构建决策树的过程为：在所有可能的特征A和其切分点a的组合中选择使上式最小的A和a将数据分成两个子集，生成两个子节点，再在子节点上重复这个过程，直到满足停止条件

以最开始的鸢尾花决策树为例：决策树做预测比较简单：就是从根节点(最上面)往下进行判断；如果现在有一个iris花,从根节点开始(深度为0)，花瓣长度是否小于2.45,如果小于2.45就是往左走，此时左边的节点没有子节点，这样的节点叫做叶子节点，然后就可以判断该花是setosa类

从上图可以看到每个节点都有一些属性(gini,samples,value,class)：

- samples属性：该节点所应用的样本数量，比如在深度为1的右侧节点中有100个训练实例的花瓣长度大于2.45，在这100个里面又有54个实例的花瓣宽度小于1.75(深度为2的左节点)
- value属性：该节点中每个类型有多少训练实例；比如最底部的右侧节点的value表示46个实例中有0个Iris setosa,1个 Iris versicolor,和45个Iris virginica
- gini属性：该节点的不纯度，如果该节点所有的实例都是一个类，那么gini就是0，表示纯的；比如深度为1的左节点，全部是setosa

该决策树的决策边界可以用下图来表示：

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210306162213679.png)

##### 估计类的概率


决策树也可以估计一个实例属于特定类的概率       
首先找到这个实例所属的叶子节点，然后返回该节点中各类的训练实例所占的比例作为这个实例属于各个类的概率；比如现在有一个鸢尾花花瓣长5cm宽1.5cm，那么它所属的叶子节点为深度为2的左节点，所以决策树输出概率为:0%是setosa,90.7%(49/54)是versicolor,9.3%(5/54)是virginica，如果让决策树来预测这个花的类别，会输出class 1 (versicolor):

```{python}
tree_clf.predict_proba([[5,1.5]])

tree_clf.predict([[5,1.5]])
```

需要注意的是：落在某个叶子节点中的所有实例的输出概率都是一样的(上面决策边界图里面同一个长方形里面的点)

#### 回归

决策树的回归也是根据某个特征来划分数据集，但是和分类不同，在划分的子集上并不是对应着一个类，而是对应着一个输出，可以用下图来理解：

<center>

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210307202506758.png) 

</center>

图中黑色的竖线代表划分(上图只有一个特征)，有颜色的横线表示每次划分后在相应的子集中的输出

假设已将输入空间(数据集)划分成M个单元(子集)：$R_1,R_2,...,R_M$,在$R_m$单元上有一个固定的输出值$C_m$,所以回归树模型可以表示为：
$$
f(x)=\sum_{m=1}^MC_mI(x\in R_m)
$$
I函数表示x在$R_m$里面的时候为1，否则为0        
在每个单元上可以使用平方误差来表示回归树的预测误差，通过最小化平方误差，我们就可以求解出每个单元上的最优输出值$\hat C_m$:

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/2021-03-07_20-40-32.jpg)

$$
\hat C_m=ave(y_i|x_i\in R_m)
$$
每个子集上的最优输出有了，那么现在的问题就是怎样进行划分？ 
对于特征j和其分割点s，(j,s)对输入空间进行划分得到两个子空间$R_1,R_2$：

$$
R_1(j,s)=\{x|x^{j}\leq s\};R_2(j,s)=\{x|x^{j}> s\}
$$
目的就是找到最优的(j,s)使得：

$$
min_{j,s}[min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]
$$
通常的做法为：遍历特征j，对固定的切分特征j扫描切分点s(如果是连续的需要离散化)，然后选择使上式最小的(j,s)组合，按照(j,s)组合对数据集进行划分，接着继续对子集重复该步骤，直到满足停止条件

在Scikit-Learn里面可以使用`DecisionTreeRegressor`类进行回归树的构建：

```{python}
import numpy as np
# Quadratic training set + noise
np.random.seed(42)
m = 200
X = np.random.rand(m, 1)
y = 4 * (X - 0.5) ** 2
y = y + np.random.randn(m, 1) / 10

from sklearn.tree import DecisionTreeRegressor

tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)
tree_reg.fit(X, y)

export_graphviz(
        tree_reg,
        out_file="../test/iris_tree1.dot",
        feature_names=["x1"],
        rounded=True,
        filled=True
    )
```

```{R eval=FALSE}
dot -Tpng iris_tree1.dot -o iris_tree2.png
```

<center>

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/iris_tree2.png)

</center>

#### 剪枝

CART算法的剪枝和一般的决策树剪枝不同

CART算法对决策树的每一个内部节点都进行剪枝，生成一个子决策树的序列；假设树的结构如下：

<center>

![](https://picgo-wutao.oss-cn-shanghai.aliyuncs.com/img/image-20210308223511075.png)

</center>

设整体树为$T_0$,对$T_0$的任意内部节点t,可以计算以t为单节点的树的损失函数$C_\alpha(t)$和以t为根节点的子树的损失函数$C_\alpha(T_t)$;当$\alpha$充分小的时候$C_\alpha(T_t)<C_\alpha(t)$(对树的复杂度惩罚较小,较复杂的树能够较好的拟合数据，因此损失函数较低)，当$\alpha$增大到某一个值的时候,两者相等，也就是单节点的树和子树的损失函数值相等,但是单节点的树比较简单,因此取单节点树,即对树进行剪枝

因此对$T_0$中的每个内部节点都可以计算一个两者相等时的$\alpha$值：

$$
g(t) = \frac{C(t)-C_\alpha(T_t)}{|T_t|-1}
$$
表示剪枝后整体损失函数减少的程度

剪枝过程就为：对$T_0$的每个内部节点计算$g(t_i)$,剪去有最小$g(t_i)$的内部节点的子节点，得到子树$T_i$,然后继续对$T_i$进行剪枝,直到根节点；对于得到的子树序列$T_1,T_2,...,T_n$通过交叉验证的方法选择最优的子树$T_\alpha$,此时也可以确定相应的$\alpha$了



